[
  {
    "url": "https://www.aboutjs.dev/en/posts/async-local-storage-is-here-to-help-you",
    "posts": [
      {
        "title": "Async Local Storage is Here to Help You#",
        "content": "When you hear the phrase \"Async Local Storage,\" what comes to mind? You might initially think it refers to some magical implementation of browser-based local storage. However, this assumption is incorrect. Async Local Storage is neither browser-related nor a typical storage mechanism. Probably one or two libraries you have used use it under the hood. In many cases, this feature can save you from dealing with messy code. Async Local Storage is a feature introduced in Node.js, initially added in versions v13.10.0 and v12.17.0, and later stabilized in v16.4.0. It is part of the async_hooks module, which provides a way to track asynchronous resources in Node.js applications. The feature enables the creation of a shared context that multiple asynchronous functions can access without explicitly passing it. The context is available in every (and only) operation executed within the callback passed to the run() method of the AsyncLocalStorage instance. Before diving into the examples, letâ€™s explain the pattern we will be using. Initialization In the module above, we initialize an instance of AsyncLocalStorage and export it as a variable. Usage The run() method takes two arguments: storage, which contains the data we want to share, and callback, where we place our logic. As a result, the storage becomes accessible in every function call within the callback, allowing for seamless data sharing across asynchronous operations. To access the context, we import our instance and call the asyncLocalStorage.getStore() method. The great thing is that the storage retrieved from getStore() is typed because we passed the Context type to AsyncLocalStorage during initialization: new AsyncLocalStorage&#x3C;Context>(). There is no web application without an authentication system. We must validate auth tokens and extract user information. Once we obtain the user identity, we want to make it available in the route handlers and avoid duplicating code in each one. Letâ€™s see how we can utilize AsyncLocalStorage to implement an auth context while keeping our code clean. I chose fastify for this example. According to the documentation fastify is: Fast and low overhead web framework, for Node.js Ok, let's get started: Now comes the very important part. We are going to add an onRequest hook to wrap handlers with the authAsyncLocalStorage.run() method. After successful validation, we call the run() method from our authAsyncLocalStorage. As the storage argument, we pass the auth context with the userId retrieved from the token. In the callback, we call the done function to continue with the Fastify lifecycle. If we have authentication checks that require asynchronous operations, we should add them to the callback. This is because, according to the documentation: the done callback is not available when using async/await or returning a Promise. If you do invoke a done callback in this situation unexpected behavior may occur, e.g. duplicate invocation of handlers Here's an example of how that might look: Our example has only one protected route. In more complex scenarios, you might need to wrap only specific routes with the authentication context. In such cases, you could either: All right, our context is set and we can now define a protected route: The code is pretty straightforward. We import authAsyncLocalStorage, retrieve the userId, initialize UserRepository, and fetch data. This approach keeps the route handler clean and focused. In this example, we'll reimplement the cookies helper from Next.js. But waitâ€”this is a post about AsyncLocalStorage, right? So why are we talking about cookies? The answer is simple: Next.js uses AsyncLocalStorage to manage cookies on the server. That's why reading a cookie in a server component is as easy as: We use the cookies function exported from next/headers, which provides several methods for managing cookies. But how is this technically possible? First, I want to mention that this example is based on the knowledge I gained from a great video, by Lee Robinson and diving in the Next.js repository. In this example, we'll use Hono as our server framework. I chose it for two reasons: First install Hono: Now, initialize Hono and add middleware: The code resembles the middleware from the Fastify example, does it? To set the context, we utilize setCookieContext, which is imported from the cookies module â€” our custom simple implementation of the cookies function. Let's follow the setCookieContext function and navigate to the module from which it was imported: The setCookieContext function (whose return value we passed to cookieAsyncLocalStorage.run() in the Hono middleware) extracts cookies from the c parameter, which represents the hono context, and bundles them with closures that provide utility functions for managing cookies. Our cookies function replicates the functionality of the cookies from next/headers. It utilizes the cookieAsyncLocalStorage.getStore() method to access the same context that is passed to cookieAsyncLocalStorage.run() when it is called. We wrapped the return of our cookies function in a promise to mimic the behavior of the Next.js implementation. Prior to version 15, this function was synchronous. Now, in the current Next.js code, the methods returned by the cookies are attached to a promise object, as shown in the following simplified example: Another point worth mentioning is that in our case, using cookies.setCookie and cookies.deleteCookie always throws an error, similar to the behavior observed in Next.js when setting cookies in a server component. We hardcoded this logic because, in the original implementation, whether we can use setCookie or deleteCookie depends on the phase(WorkUnitPhase) property stored in the storage called RequestStore(this is the implementation of AsyncLocalStorage and also stores cookies). However, that topic is better suited for another post. To keep this example simple, let's omit the simulation of WorkUnitPhase. Now we need to add our React code. The usage of cookies is similar to how it is used in Next.js React server components. Our template is rendered by the html method from the hono context. The key point here is that the route handler runs within the asyncLocalStorage.run() method, which takes cookieContext. As a result, we can access this context in the DisplayCookies component through the cookies function. It is not possible to set cookies inside React server components, so we need to do it manually:  Let's refresh a page:  And here we are, our cookies are successfully retrieved and displayed. There are many more use cases for asyncLocalStorage. This feature allows you to build custom contexts in nearly any server framework. The asyncLocalStorage context is encapsulated within the execution of the run() method, making it easy to manage. Itâ€™s perfect for handling request-based scenarios. The API is simple and flexible, enabling scalability by creating instances for each state. It is possible to seamlessly maintain separate contexts for things like authentication, logging, and feature flags. Despite its benefits, there are a few considerations to keep in mind. I've heard opinions that asyncLocalStorage introduces too much 'magic' into the code. I'll admit that when I first used this feature, it took me some time to fully grasp the concept. Another thing to consider is that importing the context into a module creates a new dependency that youâ€™ll need to manage. However, in the end, passing values through deeply nested function calls is much worse. Thanks for reading, and see you in the next post!ðŸ‘‹ PS: You can find the examples(plus one bonus) here on GitHub",
        "link": "#async-local-storage-is-here-to-help-you",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/async-local-storage-is-here-to-help-you",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/creating-reusable-ui-components-in-react-write-once-use-everywhere",
    "posts": [
      {
        "title": "Creating Reusable UI Components in React: Write Once, Use Anywhere#",
        "content": "User interfaces are evolving every day, becoming larger and more complex. We can build a lot of things â€” but to move quickly and avoid introducing bugs, we need reliable building blocks. As developers, we often work with dashboards, product lists, tables, and so on. These features require smaller UI elements that appear repeatedly and must stay consistent with the application's design. One of Reactâ€™s core purposes is to provide primitives for building user interfaces in a clear, structured way through component composition. React gives you the tools, but it doesn't solve every challenge related to reusability. In the end, itâ€™s up to you to design and compose your components in a way that makes them truly reusable. In this article, Iâ€™d like to share the best practices I follow when building reusable React components. Amid all the abstractions and syntactic sugar, even experienced developers sometimes forget what platform we're building our frontend for. Thatâ€™s right â€” weâ€™re building for the web, so we should stick to web standards. Most of our components are built around native elements like buttons, inputs, and others. These elements come with their own properties, and exposing them through props is a good practice. It gives developers more control over your componentâ€™s behavior and makes it more predictable. Letâ€™s start with this simple button: This button component has some issues that make it unintuitive to use and difficult to control: By extending the interface with web-specific types, we ensure safety because any changes to web specific props will be reflected in the interface we extend. Developers wonâ€™t have to worry about updates, and the component will remain type-safe over time. Adding a ref is also related to adhering to web standards. Without it, the component would be unusable if we want to, for example, read its element's properties like data attributes or class names. Our small modification is a big step toward reusability, but itâ€™s not enough for this component. Another crucial aspect is to allow customization. Every reusable component should be customizable, especially when it comes to styles. We already have the tools to do this. Class names â€” our good old friends from the era of static JS + CSS + HTML websites â€” and the well-known utility library Tailwind CSS. It took me some time to realize how effective this tool is. It helps us avoid overengineering styles and allows us to rely on CSS, keeping the JavaScript bundle size minimal when it comes to styling. However, when composing components styled with Tailwind, we need to be aware of potential class name conflicts and apply proper class name merging as a solution. Fortunately, we donâ€™t have to write the merging logic ourselves. â€” thereâ€™s a package called tailwind-merge specifically designed for Tailwind that handles it for us. We start by styling our components and then merge incoming className values, giving developers control over the component's appearance. Thereâ€™s no doubt that our button will need to support different variants. Since weâ€™re working with class names, we need to define styles for each variant and expose an API that lets developers choose the appropriate one. Organizing class names into variants and keep our code readable and type safe would be painful. We can quickly solve this problem using the library called: class-variance-authority. Usage: All styles are explicitly defined here, and the variants are exposed â€” everything is also type-safe. When talking about reusable components, itâ€™s common to recall the nightmare of creating flexible yet safe types. But here, everything is handled for us, and we can clearly see the shape of our buttonâ€™s style schema. It's worth mentioning that using Tailwind is optional when working with the class-variance-authority package. Another crucial aspect of creating reusable components is composability. Itâ€™s similar to customization â€” we need to give developers control over how components behave and interact. A React component should be like a LEGO block. Anyone who enjoys building with LEGO knows that some blocks can be used in many different ways, depending on how theyâ€™re combined with others. We can achieve the same kind of flexibility with our components. Sometimes, we want to bring the behavior of a specific element and combine it with our reusable component. A common situation is when you want your button to behave like a link, creating a button-link hybrid. Many libraries and meta-frameworks provide their own Link component and often we want to make it look like a button. To achieve this, we can use the asChild pattern. I first encountered this pattern when trying out radix. At first glance, it might seem complex, but itâ€™s actually quite straightforward. The reusable component accepts an optional boolean prop called asChild, along with the element that should replace its default element. When asChild is set to true, the child element passed to the reusable component is cloned and replaces the componentâ€™s default element. Their props are then merged with the parentâ€™s props. Letâ€™s implement this pattern in our Button component. Our goal is to make our Button usable in the following way: Our Button should support all the functionality that Link provides while also retaining its own properties and styles. To render an element passed as child (in our case, a Link), we need to create a Slot component: As I mentioned earlier, to achieve element swapping, we need to clone the child element. First, we must ensure that our Slot receives only one valid child. Then, we return a clone of that element, passing the ref and merging the parentâ€™s and childâ€™s props. Since our project uses Tailwind, itâ€™s a good idea to merge class names using the tailwind-merge package. Note that the props are only shallowly merged; if your component has nested props, itâ€™s better to use a deep merge pattern. Here, we keep it simple for explanatory purposes. Once we have created our Slot component, we can modify the code in Button: Now our component accepts an asChild prop, which determines whether to use the Slot component to render the child element or to render the native button element, which is the default for this component. The updated componentâ€™s code looks like this: A reusable button component is fairly easy to implement and generally not complex. However, real life is not always that simple, and we often need to build more complex components. Letâ€™s consider a slightly more complex example, like an alert. Suppose this component consists of a button that serves as a trigger to open a modal. Inside the modal, the user has options like â€œconfirmâ€ and â€œrejectâ€ â€” a common use case. We need to nest buttons inside the modal, need to use a portal, manage the open/close state, and make the component both customizable and composable â€” all without relying on many if statements. An ideal usage scenario would look like this: Everything here is composable, and the component's state can be either controlled externally or managed internally. The crucial part of our alert is the Context; we should start by implementing it. The main wrapper of the component will manage the open/close state. The component also allows optional external control by accepting a state value and a callback responsible for updating the state. This way, any child component â€” no matter how deeply nested â€” will be able to control the state. Letâ€™s add the second component: This component is responsible for rendering the content of the alert: control buttons, description, etc. The content will be placed inside a Portal. We wonâ€™t implement a reusable portal for now, so weâ€™ll use the one from radix-ui. Our component can be controlled or uncontrolled. For the uncontrolled case, we need to create a trigger component. Remember our Button? Thatâ€™s right â€” we can use it here: ...and here: Our Button component can be used in various ways, and it will still be fully customizable as a button that belongs to the AlertDialog â€” we can also take advantage of the asChild pattern. The last parts of the alert are wrappers for the description and the options (control buttons): The AlertDialogOptions component serves as a layout for control buttons, and the AlertDialogDescription is just a container for text. The patterns I presented are mostly used in radix-ui or shadcn/ui. In my opinion, those patterns are great because of their simplicity, and for React devs, they feel natural as they leverage Reactâ€™s good practices. Someone can say that can just npm install the components from one of the plenty of libraries and consume them without paying much attention to details. It's true as long as you have to implement something really custom. It is good to know why and how those things were build, in the era of code generated by AI and npm packages for everything knowledge about patterns, designs are a pure gold. I didn't cover a very important topic: testing reusable UI components. Iâ€™ll do that in the next article, so stay tuned! ðŸ’ª PS: The code is available here in this github repo.",
        "link": "#creating-reusable-ui-components-in-react-write-once-use-anywhere",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/creating-reusable-ui-components-in-react-write-once-use-everywhere",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/from-user-to-contributor-my-journey-with-cookies-next",
    "posts": [
      {
        "title": "From User to Contributor: My Journey with cookies-next#",
        "content": "In the past, I had many opportunities to work on authentication systems. Everyone knows that when it comes to authentication, cookies often need to be handled, and it's helpful to have tools that streamline working with them. One such tool is cookies-next, a tiny npm package designed (as its name suggests) to work with cookies in Next.js. As far as I know, this package is the only tool available for cookie management in Next.js that supports v15+. In my opinion, as well as in the opinion of others, cookie management in Next.js can be challenging due to the architecture of the framework, which combines both server-side and client-side. I will show you how cookies-next simplifies cookie management in Next.js and provide some insights from being a contributor to this library. The API is quite simple. On the server, itâ€™s required to pass the context or the cookies function imported from next/headers along with optional cookie options to one of the asynchronous functions whose names correspond to the operations: set, get, and delete. On the client, dedicated hooks are available, or you can use the same functions as on the server, but in their synchronous versions (without passing any context or the cookies function imported from next/headers â€” just the optional cookie options). Below are a few examples: Client Server I used cookies-next for the first time while working on a new project. The decision was made to use Next.js v13+ with the app router, which was a very new feature at the time. My task was to build an authentication system. In this system, one of the tokens was stored in cookies. There were also other features that used cookies and needed to access them on the client side. As a result, we needed consistent APIs to manage cookie storage. However, Next.js didnâ€™t have a consistent API that could handle cookies both on the server and the client. This made cookies-next the perfect solution for this project. In the examples below, we will compare cookie management in Next.js with and without cookies-next to see how cookies-next solves the cookie management issues I encountered. Start by setting a cookie on the server using only the Next.js API: And hereâ€™s the first problem: to avoid hydration errors, we must ensure that the code does not cause a mismatch between the HTML rendered on the server and the HTML rendered on the client. In a growing codebase, this requirement forces us to create a custom hook to efficiently use this feature. We also encountered other disadvantages, such as: These small differences can cause significant confusion and hinder the development process. Letâ€™s move forward and see how to read the same cookie in a React Server Component as well. Again, we have to go back to the Next.js API, which is not consistent with the js-cookie API we use on the client. Letâ€™s see how it looks with cookies-next: We pass only the arguments required for setting a cookie. On the client, all we need to do is import a hook that returns an appropriate getter function: And now let's read our cookie in the server component using cookies-next. The library takes advantage of TypeScript features and guides us on what we should pass to setCookie. We don't need to do anything more than call the function and pass the required arguments. The examples I presented above reflect the current state of the library. Back when I decided to use cookies-next, there was no support for the app router and server components. Recognizing the need for this feature in the project I was working on, I decided to add it myself. The story began when I installed cookies-next, hoping it would provide all the features I needed. But, of course, it wasnâ€™t that simple. I tried using this tool in the Next.js middleware, and it broke. I quickly realized that cookies-next didnâ€™t support middleware and other Next.js app router features. It could only be used in client components and pages router. I decided to check for other packages, but I encountered at least three issues: I returned to cookies-next and delved into its repository. I discovered that the code was short and straightforward, so I added support for Next.js middleware, enabling cookies-next to fully support methods attached to NextResponse and NextRequest. This allowed me to use the package in my project without any issues. Some time later, I came up with the idea of adding support for other new features of Next.js v13+ by enabling the library to use the cookies function imported from next/headers. If you're keen to dive into those updates, here are the PR's: Middleware support Extended support for Next.js v13+ After the release of Next.js version 15, warnings began to appear. The cookies function from the next/headers module, which is sometimes needed in cookies-next for handling cookies on the server, became asynchronous. There was a need to integrate cookies-next to handle asynchronous functions on the server and synchronous functions on the client. One of the library's users raised this issue and submitted a PR, in which this user, the maintainer of the package, and I worked together to resolve it. Server-side cookie function from cookies-next before and after the Next.js v15 update: When trying to make cookies-next compatible with the new version of Next.js, we encountered typical edge cases related to the dual nature of Next.js. The idea was to provide users with an API that allows them to explicitly choose whether to import the client or server-specific cookie function - import { getCookie } from 'cookies-next/server'/or /client, or to leave this decision to the library (which we call a 'smart import') - import { getCookie } from 'cookies-next'. In both cases, we needed logic to determine in which phase of rendering the cookie function is called, without causing rendering-specific errors in React components. Initially, it might be obvious that we can determine which environment is currently in use by using a check like this: But it's not enough, and it usually causes hydration errors in the client components. Everyone who has worked with the Next.js app router knows that the client component pre-renders on the server, and during this phase, window is undefined. In our case, relying only on window when deciding which function to use (for the server or client) could lead to unexpected errors, such as serving asynchronous functions to a client component. We came up with an idea to address this problem: Relying on the context passed to the cookie function In typical JavaScript server code, to set a cookie, we need a context (such as request and response), which usually includes methods for cookie management like set, get, etc. Next.js also follows this paradigm, so, in general, on the server side, we have at least two options for setting cookies: There is no need to pass any context in the client-side case, as cookies-next simply utilizes the document.cookie API. Has something clicked in your mind? That's right! Based on all the information above, we can determine the current environment by checking if a context has been passed. So, let's rewrite the examples to use cookies-next: The object passed to the cookie function includes the context (request, response, or asyncLocalStorage for cookies). Inside the cookie function in the library code, we can perform checks like: The last major task we needed to address with cookies-next was making it React-friendly. Obviously, most libraries used in React come with hooks ready to consume their API. However, cookies-next didnâ€™t include them, so I decided to add them. Implementing hooks in this case provides two major benefits: In the example below, the getCookie function causes a hydration error because, even though cookies-next determines that the client environment is in use by checking for the absence of context, it still has to wait for document.cookie to become available by performing a check like const getRenderPhase = () => (typeof window === 'undefined' ? 'server' : 'client');.  The solution to this is hooks. If we make the cookie function rely on the React state, the error is gone: Itâ€™s a bit too much code to get the helper function from the library to work, isnâ€™t it? Therefore, I added similar logic to cookies-next, and you can use it as shown in the updated example below: Iâ€™m not in favor of using an npm package for everything; sometimes, I prefer to write my own utils to have full control over my code. However, in cases like cookie management in Next.js â€” where there are many potential pitfalls â€” Iâ€™d rather choose a library that works out of the box, like cookies-next. What I love about this library is its simple API, which handles all the obstacles related to the cookie API and the server-client nature of Next.js. Without it, my team and I would have to handle all those edge cases ourselves. Community support is also a key factor. When I create my own utils, I must take care of every dependency related to them. If one of the dependencies releases a new version with breaking changes, Iâ€™ll have to handle it myself. But when I use a library with an active community and good support from maintainers, I can get involved and work with them to deliver updates more quickly and safely. A great example of this is, again, cookies-next, where we delivered updates that followed breaking changes in Next.js. So, donâ€™t hesitate â€” pick cookies-next for cookie management in your Next.js app. Thanks for reading, and see you in the next post!ðŸ‘‹",
        "link": "#from-user-to-contributor-my-journey-with-cookies-next",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/from-user-to-contributor-my-journey-with-cookies-next",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/implementing-authentication-in-nest-js-with-drizzle-and-passport-js",
    "posts": [
      {
        "title": "Implementing Authentication in NestJS with Drizzle and Passport.js#",
        "content": "NestJS is a backend framework that stands out from other Node.js frameworks. Unlike Express or Hono, which offer complete freedom in structuring the application codebase, NestJS provides a more opinionated approach, explicitly guiding how your application should be organized. Despite mixed feelings among developers in the JavaScript community, NestJS continues to grow in popularity, securing second place in the backend frameworks category of the State of JS 2024 survey. NestJS enforces code organization using well-defined building blocks, typically structured into modules based on features or domains, such as users, authentication, or products. Each module directory generally consists of at least three key files: a service, a controller, and a module. The service handles business logic, the controller manages requests, and the module establishes boundaries and rules for how different parts of the feature interact. One of NestJSâ€™s most important features is dependency injection. Each reusable class, such as a service, is marked as injectable using the @Injectable() decorator. This allows NestJSâ€™s DI system to inject it into another classâ€™s constructor â€” for example, a service can be injected into a controller The @Injectable() decorator acts as metadata that the DI system reads at runtime. Magic, isn't it? In our controller, we only need to import the UserService, and NestJS will inject it into the UserController, making it available via the constructor. However, thereâ€™s one more step to make it work. As mentioned earlier, we need to explicitly define rules and boundaries for the classes inside the users directory by creating a users.module.ts file. Having gained a high-level understanding of how NestJS works, we can now begin building our mini authentication example. We'll use Google as the identity provider (IdP), manage authentication strategies with passport.js, and store user data in PostgreSQL, drizzle will serve us as an ORM. Install NestJS along with the required dependencies. After installation, the initial project structure should be set up and configured. For Google authentication, we need to store secrets in the .env file. These secrets can be obtained from the Google Developer Console. To store user data, we need to create a database and connect it to our application. We'll use docker-compose for this. Create a docker-compose.yml file in the root of the project. Don't forget to add the database connection to your .env file. As I mentioned earlier, we will use drizzle as our ORM. Since we are using PostgreSQL, we also need to install the pg package. Additionally, we can install drizzle-kit to get a nice GUI for managing the database. The next step is to configure Drizzle. Create a drizzle.config.ts file in the root of the project and define the configuration. Set up the paths for the schema and migrations, and specify the database connection URL. Now, create a directory called drizzle inside src, and add a schema.ts file to define the structure of our database data. We also need to create a migration script. Now, we can finally dive into some NestJS magic. To use our drizzle-powered database connection, we need to define a provider called drizzleProvider. As far as I know, drizzle does not offer built-in NestJS integration that includes such a provider, so we have to create a custom one. Custom providers in NestJS act like arguments passed to a constructor. At runtime, NestJS reads the token and injects the provider into any class where the DrizzleProvider token is explicitly added to the @Inject() decorator. The role of the useFactory function is to create the value dynamically while having access to injected dependencies â€” in our case, the ConfigService. We need to pass several properties to the object stored in the providerâ€™s array: The final step in setting up Drizzle is to create the DrizzleModule. Once the provider is set up, we need to register it in app.module.ts and make it globally available in our application. Since our authentication is built on top of Googleâ€™s service, we need to create a Passport Google strategy for it. First, install the required dependencies, such as Passport, passport strategy for Google, and utilities for NestJS. Our Google strategy is simply an injectable extension of the PassportStrategy class designed for NestJS. Almost everything is handled out of the box for us here. We just need to pass the configuration with the Google auth credentials and define the validate method, which in this case extracts and formats the Google user data. While adding validation (e.g., using a library like Zod) might be a good idea, we'll leave it as-is to keep this example simple. We also take advantage of Dependency Injection to retrieve the credentials from the ConfigService. To use our strategy in a controller, we need to define an extension of the class that enables us to do so, which is the AuthGuard. This class acts as the trigger and encapsulates all the authentication logic. We don't need to implement any custom solution here, as we have the battery-included class exported from the @nestjs/passport utility package. Once we have our strategy and guard prepared, we need to add them to the auth.module.ts file, within the providers array, to make them available for dependency injection within the auth module. To perform authentication, we need to implement logic for storing and managing users. So, let's create a class called UserRepository in the user.repository.ts file. In this class, we have several methods that manage the user entity. The interesting part is in the constructor definition, where we pass the DrizzleProvider token to the @Inject() decorator to tell NestJS to inject the value into the class. I mentioned this earlier in the database setup section. Once this is done, our db parameter will hold the database connection. You might have noticed that our custom drizzle provider is not a class. If it were, we wouldnâ€™t need to use the @Inject() decorator, as NestJS DI system would know which class to inject by simply looking at the type assigned to the constructor's parameter. Letâ€™s add our repository class to the module exports. Additionally, we need to ensure that our custom provider is imported correctly. Before we create the authentication service and add controllers, let's add an additional Passport strategy to manage the session of the logged-in user. Install the required dependencies for this: Add the secret required for access token creation: Next, define the JWT strategy. We will save the token in a cookie, so in the strategy, we need to define how it can retrieve the token using the ConfigService injected into the strategy. The token will be transformed automatically. In the validate method, we can call one of the methods from UserRepository to perform a simple validation of the token by checking if the user assigned to the token exists in the database. To use our strategy, we need to define an auth guard, following the same pattern as we did with Google authentication. Update auth.module.ts Along with the JWT-related class, we need to add our UserModule to the imports array to be able to use the UserRepository inside the auth module. The service layer usually contains business logic, so we should place all the logic related to user registration, login, logout, user checks, and token creation there. The AuthService utilizes three injected classes. The patterns that NestJS offers allow for composing code like Lego blocks. Now it's time to make our application usable. We will create two controllers: Let's start with the AuthController. To define a controller, we use the @Controller() decorator, passing a string that represents the route's name. We inject the AuthService into the controller as we need it to handle the authentication logic. Next, we use the @Get('google') decorator to define the entry point for login. Below that, we add another decorator: @UseGuards(GoogleGuard) to apply the guard prepared earlier. This tells NestJS to trigger the authentication logic. Afterward, the user is redirected to the Google sign-in screen. Upon successful login, the user is redirected to google/callback, where the guard handles the validation and communication with the Google IDP to retrieve the user's profile information. After this, we can call the signIn method to generate our applicationâ€™s access token and then redirect the user to a protected route with the token assigned to the cookies. Once we finish our AuthController, we need to register and export it from the module. So, update the auth.module.ts file. Letâ€™s also export the JwtAuthGuard, as it will be required in the user module to protect routes. Following the user's path, we create UserController to retrieve and return users from protected routes. As in the AuthController, we will use guards here, but not for authenticating a user â€” rather, for protecting routes. If the token is invalid, none of the functions in the controller will run, and an UnauthorizedException will be thrown instead. Once we cover all cases and create all the building blocks of our application, we need to add the AuthModule and UserModule to app.module.ts. At first glance, Nest.js might seem a bit complex and difficult to grasp, but once we dive into its core concepts, we can see how easy it is to build with. For cases where we need to quickly spin up a small server, it may not be the best choice, but for large projects that need to scale well, this framework is an excellent solution. Developers familiar with Nest.js can quickly become productive when joining a project built with it. The framework also encourages a well-structured codebase. Without a doubt, Nest.js will continue to grow in popularity, making it a solid choice for kickstarting scalable Node.js backend applications. Thanks for reading, and stay tuned for more articles! ðŸ‘‹ PS: You can find the project's code here on GitHub",
        "link": "#implementing-authentication-in-nestjs-with-drizzle-and-passportjs",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/implementing-authentication-in-nest-js-with-drizzle-and-passport-js",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/managing-the-context-window-of-gtp-4o-mini-in-javascript",
    "posts": [
      {
        "title": "Managing the Context Window of GPT-4o-mini in JavaScript#",
        "content": "GPT-4o-mini is a powerful, fast, and affordable language model that can be used for a wide range of tasks in different languages. The context window of this model is 128k tokens. You might think that this sounds like a lot, but when we compare it with things like an excessive prompt consisting of a complicated schema, a few-shot example, and the content of the document you want to analyze, along with the LLMâ€™s response, we will quickly realize that it is not that much. In this post, we will dive into this topic by building a simple keyword extractor using GPT-4o-mini and Node.js. Stability and predictability are among the most important aspects here. Even if the model's context is large, the API may have limitations on tokens per minute. Model response accuracy and performance are also crucial. Sending all of your text for analysis at once is not the best idea, as the result will likely be poorer because the model will have to focus on a broader context. Sending more requests in parallel leads to faster analysis and higher-quality responses. With these aspects in mind, we can adjust the size and number of requests sent to the model to meet your applicationâ€™s requirements and stay within the limits. A token is the smallest unit of text data. A word is split into fragments. The process of splitting text is handled by the tokenizer. The way in which the text is tokenized varies depending on the LLM model and its tokenizer. For more explanations about tokens, you can refer to this OpenAI article. The context window refers to the number of tokens that the LLM can handle at the same time. It's like memory. Imagine you're writing a summary of a book. You can only process a window of 50 pages in your mind; the previous pages are forgotten. This is how the context window works, especially in the case of the keyword extractor we're going to build. In our case, we will focus on a single text input composed of: prompt, output schema, and text for analysis. This approach requires different strategies than maintaining a conversation(chat), such as prompt-response. The goal of our application is to extract keywords from the text, it may be an article, book, web page etc.. We have to be prepared for very long texts, so the context window management is crucial. To effectively fulfill the requirements of our application and ensure its stability, we must pursue the following strategies for managing the context window: Prompt strategies: We can control the output by including guidelines in the prompt. In our case, we can instruct the LLM to output only keywords with a maximum of 2-3 words and return only the 5 most relevant ones. This is important because the OpenAI API does not allow setting a maximum number of items in the schema for structured output. Counting tokens: To count tokens, we will have to choose a tokenizer and implement counting logic before sending the text to the LLM. Additionally, we must estimate how many tokens the output will contain. Chunking the text: Chunking text ensures that we don't exceed the context window. In our case, the need for contextual awareness is less important, so we can use more aggressive chunking strategiesâ€”sending more but smaller chunks to the LLM while ensuring that words and sentences are not cut off. Structured output: Forcing the LLM to output structured data and setting the temperature to 0 for a more concise output also gives us control over the context window, as we can easier estimate how many tokens the output will have. The first thing we need to have in place is prompts: As I mentioned earlier when talking about prompt strategies, we can explicitly tell the LLM how long the keyword list should be and how long each keyword can be. We end our main prompt with the text Input text: because we will concatenate the input text with the main prompt later in the code. Once we have the prompts prepared, it's time to define our schema for the structured output. We will use zod. zod works perfectly with the withStructuredOutput method from the langchain library, which we will use to manage the LLM's API requests. langchain converts the zod schema into the JSON schema accepted by the OpenAI API. Additionally, we can take advantage of the types for our outputs, which are automatically inferred from the schema. Now it's time for the first part of the logic: we're going to split the text into smaller parts In our example, the chunk size is set to 2000 tokens. While the entire context window of GPT-4o-mini is 128k tokens, we could technically set the chunk size close to this value, but a smaller chunk performs better in terms of both performance and output quality. Here is how we declare our function for this: The first thing we need to do to determine how to split the text is perform calculations. For this, we need to use a tokenizer to encode the text into tokens. In our case, js-tiktoken is the best choice because it is compatible with the model we're currently using. To ensure the best estimations, we need to subtract the tokens used by the prompts, schema, and text from the maxChunkSize parameter. Our schema is in the zodType format, so we need to convert it to a string. For this, we use the zod-to-json-schema package. As a result of the calculation, we obtain the final chunk size. We provide two values to the splitter's constructor: For our application, we use RecursiveCharacterTextSplitter, which chunks text from sentences down to words. This means that the algorithm first checks if one or more phrases fit within the chunk size threshold. If they don't, it recursively breaks the text down into smaller parts. We have a function for splitting the text in place, so we can start sending requests to the model. The first thing is the configuration. We use langchain's integration for OpenAI. It is worth mentioning that by setting the temperature to 0, we can indirectly influence the LLM's response, making the output concise and preventing the model from being too creative. Next, we declare a function for calling the LLM. We need to preprocess our data and prompts, so we load a document with the text and chunk it using the function we prepared earlier. For the purpose of our example, we simply load a file with the text, but the text could also be passed to the function via arguments. Now we can register our zod schema to ensure structured output from the LLM and send the requests. We can take advantage of chunking by running them in parallel and awaiting the responses using Promise.all. This allows us to process the data faster and in smaller parts, helping the LLM focus more on details, which leads to better results. However, remember that when sending requests this way, you need to account for API rate limits. The last step is output formatting and data filtering. We merge responses from each chunk and filter them based on the confidence level to ensure that only relevant keywords are returned. Finally, we remove duplicates using new Set(). Managing the context window is crucial, especially when focusing on large text analysis. Rate limits can be tricky, and despite the model having a 128k token window, these limits can prevent full utilization. For example, currently, for GPT-4o-mini hosted on Azure Open AI, the max initial quota is 30k tokens per minute! To get more, you have to request a larger quota from Microsoft. Even if your limits are higher, maintaining control over the context window is essential, whether for a production-grade application or a fun project. No one wants to run their AI integrations while constantly encountering errors. Thanks for reading ðŸ™Œ PS: Look at the repo of the keywords extractor app here on GitHub",
        "link": "#managing-the-context-window-of-gpt-4o-mini-in-javascript",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/managing-the-context-window-of-gtp-4o-mini-in-javascript",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/not-just-testing-react-ui-components-with-vitest-and-storybook",
    "posts": [
      {
        "title": "(Not Just) Testing React UI Components with Vitest and Storybook#",
        "content": "In my previous article, I focused on building reusable UI components in React. This post continues that topic, with an emphasis on testing. I have the pleasure of working in web development on both the server and client sides, and I often say that frontend can sometimes be far more unpredictable than backend â€” and the code can quickly turn into a mess. To reduce the number of headaches, it's important to test our UI code properly. In this article, weâ€™ll explore how to do exactly that. When talking about testing, it's good to start with unit tests as a little warm-up. The idea is simple â€” a unit test should be performed on the smallest possible part of the logic, such as a single function. We will use vitest as the test runner and @testing-library/react to access the React API in tests. Before we start, letâ€™s create two small utility modules that we'll use in our tests. Now we can move forward and write tests. Remember the Button component from the previous article, where our journey began? In the Button component, we use the asChild pattern. It might be a good idea to test this behavior, as it determines how the user interacts with our button. Consider a case where we want a file upload input to look like our Button. So, a basic unit test for this case might look like this: Simple components like our Button usually donâ€™t have many use cases worth testing with unit tests. Itâ€™s clear that most user interactions happen across multiple components working together, which is where integration tests come into play. This approach, instead of focusing on the smallest parts of the application, targets the integration of multiple components. Consider a list component. Each list item has two buttons: one for adding and one for deleting. If the list is empty, a message indicating the empty state is displayed. The ListItem component uses reusable components like Button and AlertDialog. For our example, we will focus only on the deletion action to concentrate on testing user interaction with these components. The behavior is well known: the user clicks the delete button, and an alert dialog appears with an additional prompt and two button options. I mentioned the \"empty state\"; now, letâ€™s define a component for rendering it: The logic for deletion and state management is handled by the ListExample component. Once the user clicks on an itemâ€™s delete button, an alert dialog appears. The user can either cancel the deletion or confirm it. If confirmed, the item will be removed from the list. If the list becomes empty, the EmptyState component will be rendered. With our components defined, we can write tests. We will test a few user scenarios: Each component has its role and contributes to the overall experience of the ListExample. Testing how those components work together is valuable because it allow us to test real user action scenarios. But what if we want to see interactions on the screen without running the entire app, or if we are building a design system completely independent from any application? This is where Storybook shines. When developing frontend code, tests that only output results in the terminal are often not enough. UIs are what users see and interact with. Components often not only behave differently but also look different when they receive different props or appear in various layouts. Itâ€™s good to have them isolated and be able to easily change their input. Storybook is a tool that allows developers to build UI components in isolation. Its features facilitate the development process, from creating and debugging to manual and automated testing. Letâ€™s focus on some Storybook features that can streamline the process of testing and developing UIs.s Stories are an excellent feature for documenting components and testing them manually in isolation. They allow us to test the user experience by interacting with components just as users would. Stories also serve as a place to define the conditions under which the component is rendered, making it easy to create a gallery of the same component in different scenarios. Let's write the docs for our Button component: One of the things we should do is document our component. In the era of AI coding editors like GitHub Copilot, Cursor, and others, adding documentation is mandatory. You can simply generate the documentation and quickly review it. This will be especially helpful for your team if the component is complex. The API of Storybookâ€™s Docs is intuitive and easy to understand. The result of the code above looks quite neat:  We can visually inspect our component, interact with it, and dynamically change its props using controls. Any changes to the component are instantly reflected in the preview. Alright, we have the docs â€” now letâ€™s define stories for our component: Stories are objects that describe the appearance and behavior of our component. Defining a story is like calling the component with specific props. It is very useful, especially in the case of our Button, to test how it looks and behaves when using the asChild pattern. Each exported story will be available in the Docs tab, in the \"Stories\" section under the main component section:  But to interact with each of them separately, we need to go to a dedicated view accessible via the navigation. Let's choose the \"Upload\" story of the Button:  Developing with stories helps developers catch many bugs early. Itâ€™s similar to developing with TypeScript â€” you can catch issues before even running your appâ€™s code. There is no doubt that stories are a powerful tool, but to complete the testing workflow, we need automation. Fortunately, Storybook gives us the ability to write automated tests. This feature is available via play function that we can define in our stories. Letâ€™s switch to our ListExample component and write tests for it. As you can see, some code patterns and test syntax are similar to what we already know. Test cases are included within stories. The stories we defined above are reusable steps that reflect the testing process. These initial three stories are intentionally not exported because we donâ€™t want them to appear in the navigationâ€”they are used only as reusable, composable parts for the integration tests below: At the bottom of the preview, we can find a tab called \"Interactions\" that allows us to go through each defined case manually or play them from the start.  We can interact with this list by jumping to a specific step. The state of the component will also be visually reflected in the preview.    As you can see, for our Button used as an upload control, some tests have passed, while some violations have also been detected. Such feature can be a great starting point for detailed accessibility testing. With Storybook, we can even share our design system. Sharing the progress of work is very important for stakeholders and people on the business side. In conversations about business requirements, clear examples and visible outcomes are key to preventing misunderstandings, which makes iterations faster and more effective. Storybook offers decent sharing capabilities, such as building and publishing it as a static web app or integrating with Figma. Testing and building with the right tools can save us a lot of time. Moreover, imagine developing design systems with strong involvement from AI agents and testing them in Storybook â€” it's a powerful combo, isn't it? With a properly configured testing environment, we can focus on solving real business problems without spending an excessive amount of time fixing bugs. However, remember that Storybook is not always the best choice. Sometimes, using it in smaller projects can be overkillâ€”the same applies to unit tests or integration tests. When your application is small or is just a static website, it can be faster and still safe to test manually or only with end-to-end tests. When I was planning this article, I was lucky to find a great episode of the Syntax podcast featuring one of the Storybook team members. If you want to learn about Storybook from its teamâ€™s perspective, itâ€™s definitely worth a listen. PS: Syntax is my favorite web development podcast. Thanks for exploring this topic with me! Stay tuned! PS: If you want to try out the code from this article, check out this GitHub repo.",
        "link": "#not-just-testing-react-ui-components-with-vitest-and-storybook",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/not-just-testing-react-ui-components-with-vitest-and-storybook",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/one-user-many-services-linking-dropbox-with-google-in-nestjs",
    "posts": [
      {
        "title": "One User, Many Services: Linking Dropbox with Google in NestJS#",
        "content": "In an era where we have plenty of services for almost everything, there is often a need to automate them or gain clear insight into the resources stored within them. From a developerâ€™s perspective, there is a need to build abstractions that allow users to connect those services to their account in the application you're building. Fortunately, we have OAuth, a well-established standard on the web. Letâ€™s dive into an example and build a feature that allows users to connect a Dropbox account to an account created via Google. We will build this example on top of a project that already implements a Google authentication flow. I covered this use case in the previous post, of which this post is a continuation. To start coding, you can clone this repo and switch to the branch called only-google-auth. Our goal is to allow users to connect multiple Dropbox accounts. To achieve this, we need to set up: After implementing the account-connecting pattern, weâ€™ll have a solid foundation for a service that can support additional features â€” for example, collecting and organizing files from Dropbox and Google Drive, or allowing AI agents to access usersâ€™ resources stored across both services. Alternatively, it could simply act as an auth/proxy service for another application that requires account linking. We use AES-256-GCM for symmetric encryption. Remember to add the ENCRYPTION_KEY to your .env file. We need to update our database schema to store the Dropbox data and maintain relationships. The application uses Drizzle to handle database operations, so this task will be easy and straightforward. Weâ€™ll create two new tables: Additionally, we add a column named is_connected_to_dropbox to the user table to mark users who have their Dropbox accounts connected. Choosing a many-to-many relationship between users and dropbox_accounts allows our users to connect multiple Dropbox accounts to their accounts in our application. This approach is flexible and scalable. To connect a Dropbox account to the account created via Google, we need to provide users with a way to authenticate their Dropbox account. This process will be almost identical to the standard OAuth flow. We need to send the ID of the user to the Dropbox auth service and receive it back. To handle this, we can pass a JWT token with the userId in the payload to the state parameter of the authorization URL. To pass the user ID to the state parameter, we need to make it available in the context of our Dropbox authentication. To achieve this, we must extract the user object from the execution context and pass the user ID as the state. We can accomplish this in the DropboxGuard. Once we have the userId returned from the guard, we can use it in the DropboxStrategy, which is a Passport.js-powered authentication strategy. Let's install passport-dropbox-oauth2: and define the strategy: Now we can clearly see where we send the state and when we get it back. To send the state, we get the userId (returned from the guard) from the context and then create a short-lived JWT token that contains the userId. Then, include the token in the state parameter of the authorization URL that is generated for the user after they hit the auth/dropbox endpoint. The state will be available when the auth service redirects the user to our auth/dropbox/callback endpoint. To access the state in the validate method of the Dropbox auth strategy, which runs during the auth callback, we need to set the passReqToCallback: true flag in the constructor's configuration object. Inside the mentioned validate method, we can read the state parameter and extract the user ID from the JWT token using the validateStateToken from the AuthService. After validation, the Dropbox account data will be passed to the done callback, making it available for further processing. Now we will add our Dropbox auth logic to the controllers module. We already have the main authentication and token flow implemented, so we just need to connect our Dropbox auth logic to it. The starting point looks like this: The first thing we need to add is an endpoint to initiate authentication, along with middleware that acts as a guard to ensure only authenticated users can connect Dropbox account to their existing accounts in the application. The important thing here is that we combine JwtAuthGuard and DropboxGuard, passing them to @UseGuards. Guards act as an authorization layer, and passing them one after another tells NestJS to first execute the code for checking the user's access to the application, and then activate the Dropbox authentication. The DropboxGuard also retrieves the userId from the execution context and passes it to the DropboxStrategy, as we covered in the previous section of this article. After the initial authentication, the Dropbox IdP provider will redirect the user to the authentication callback endpoint. At this point, we use the DropboxGuard again to activate the validate method form the DropboxStrategy, along with the state we sent earlier. With all the data required for the connection, we can trigger the Dropbox connection logic handled by the AuthService. The last steps we need to undertake are: We need to add the following logic to our existing AuthService: First, we need to check if the received data fits our schema, parse it, and infer the type from it. For this task, zod is a perfect choice. If something is wrong with the data shape, we should throw an error early. The next step is to check if the userId corresponds to the target user's ID already stored in our database (the userId came from the state parameter). If not, we should also throw an error. Once all checks pass, we can update the records in the database to reflect the account's connection. We use our EncryptionService, created earlier, to encrypt Dropbox tokens. When it comes to the dropbox_accounts table, we update the database using the upsert pattern (ON CONFLICT DO UPDATE ...) in case of reconnecting, e.g., when the refresh token is not valid and the user reconnects. Additionally, it is good to wrap all queries in a transaction to ensure consistency. If any errors occur while executing the queries, the entire transaction will be rolled back to the state before the transaction. We returned to NestJS to add a new feature, and it was surprisingly seamless. We could safely add new authentication as if it were another brick in the wall. Even in such a complex case, we were able to break down the authentication process into manageable steps. We can even connect accounts from more providers in a similar way, gaining access to more user resources distributed across different services. In my opinion, NestJS is definitely a technology to be aware of if you're into JavaScript web development. The best way to grasp a concept and learn new things is by building. So, clone the repo, add some features (for example, pulling users' resources from Google and Dropbox or handling Dropbox token refresh), and NestJS will never seem complex to you again (if it ever was). Thanks for exploring this topic with me ðŸ™Œ",
        "link": "#one-user-many-services-linking-dropbox-with-google-in-nestjs",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/one-user-many-services-linking-dropbox-with-google-in-nestjs",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  },
  {
    "url": "https://www.aboutjs.dev/en/posts/zod-the-quiet-hero-of-modern-web-development",
    "posts": [
      {
        "title": "Zod: The Quiet Hero of Modern Web Development#",
        "content": "When talking about the most popular tools in JavaScript web development, we often hear names like React or Express. But thereâ€™s one tool thatâ€™s usually mentioned in passing â€” a standard addition to those. That tool is Zod. Its simplicity and the obvious need for runtime type checking make it a natural fit. According to Zod's page on npmjs, the first version was released in 2020. Zod now gets over 35 million weekly downloads on npm, which is pretty impressive when you compare it to heavyweights like Express with around 45 million, or React with about 41 million. When we look at Zod's documentation, we learn that: Zod is a TypeScript-first validation library. In plain English: Zod does the job that TypeScript canâ€™t. TypeScript doesnâ€™t validate types at runtime â€” type safety is only available during transpilation. Zod fills that gap. Often we deal with data that isnâ€™t typed â€” like the data returned from a fetch call to the server. To ensure data flow integrity within our application and prevent bugs, we need to verify that the received data matches the expected schema on the client. With Zod, we have at least two ways to do that: or To infer static types from the schema: Moreover, Zod allows us to compose schemas in many ways â€” supporting various data types, nesting, and even self-referential types all of which is accessible through an intuitive, clean API that lets us define schemas in a declarative way. In my view, the most crucial reason why Zod is so popular is its excellent integration with TypeScript. It offers robust performance and a clean, well-designed API. TypeScriptâ€™s growing adoption helped Zod rise in popularity â€” being an extension for one of the fastest-growing programming languages naturally brings a lot of attention. Other important factors: enhances code execution safety â€” Zod acts as your guard when it comes to data compatibility, providing a robust tool for controlling your data during code execution. It also works perfectly with observability tools. When part of your logic encounters bugs caused by the wrong data type, Zod will catch and report them. This is especially useful when integrating with external services, when you donâ€™t have control over their API code. can be extended â€” You can handle even the most complex use cases. If there isnâ€™t a type that fits your needs, you can take advantage of Zodâ€™s features like .refine, .superRefine, and others. intuitive â€“ Zod is highly intuitive for any TypeScript developer. Its methods reflect TypeScriptâ€™s features and type names. You can even use familiar utilities like pick or omit. The fact that Zod follows well-known patterns from other validation libraries (such as chaining methods from a single imported object) allows developers to start using it seamlessly. It is a crucial incentive to switch to Zod for a developer who expects great TypeScript integration from a validation library but has no prior experience with Zod. â€“ Easy to integrate â€“ Zod can be seamlessly integrated with many frameworks and libraries. It can be used on the frontend, backend, for validating AI-generated output, requests, interactions with databases, and more. This versatility across different parts of the stack has had a strong impact on Zod's popularity. Zod is useful in many different use cases â€” on both the client and the server. Letâ€™s explore three common scenarios, excluding the obvious one: validating fetched data. Every frontend developer knows that form data validation is extremely important. This use case can be complex, especially when the application relies heavily on user input. Poor form validation can ruin the user experience. Zod fits perfectly in this use case. If youâ€™re building your form in React, thereâ€™s a good chance youâ€™re using react-hook-form to manage it. Zod can be integrated with it through the resolvers feature: How easy is it? All we need to do is define the schema â€” the rest is handled for us. User input will be automatically validated by Zod, and if the data is invalid, Zod will generate errors. Every developer knows that validating data coming from the client is crucial for the security, consistency, and stability of a backend application. When our application receives a payload from the client, we donâ€™t actually know what kind of data it is. Consider an example of a validation middleware in one of the most popular unopinionated backend frameworks: Express. Zod does a few things in this case: it validates the transferred data, strips unrecognized properties, handles validation errors, and infers types. Matt Pocock, in one of his videos, said that: ...but is[Zod] now also getting a second life in the AI space where people are using it to do structured output. Structured output is useful when AI needs to be integrated with an application that expects output to be structured and deterministic â€” meaning the data should always have the same shape. This is the case, for example, when we prompt an LLM and expect the output to always be a JSON with defined properties. Structured output is now a standard option in the APIs of the most popular LLMs. We can use a combination of Zod and LangChain to force the LLM API to respond with structured data: We create a schema using Zod, then use it to tell the LLM how the output should look. Once the LLM API responds, LangChain validates and strips the output to fit the expected schema. In the end, the result becomes typed. Having integrations like Zod with TypeScript shows the direction of the JavaScript ecosystem. JavaScript has come a long wayâ€”from a trivial scripting language for simple web interactions to a language equipped with great tooling that allows us to build full-stack isomorphic applications. Zod itself keeps improving; its recent v4 update introduced new features, enhancements to existing ones, and a performance boost. You can check out a quick Matt Pocockâ€™s summary video on Zod v4 mentioned earlier. Thanks for reading! Follow along for my upcoming articles â€” youâ€™ll definitely see more examples with Zod hidden in my future code snippets. ðŸ‘‹ PS: Checkout my GitHub repo with the examples I presented in this article.",
        "link": "#zod-the-quiet-hero-of-modern-web-development",
        "date": "No date",
        "source": "https://www.aboutjs.dev/en/posts/zod-the-quiet-hero-of-modern-web-development",
        "selector": "http-parser",
        "index": 0
      }
    ],
    "success": true
  }
]